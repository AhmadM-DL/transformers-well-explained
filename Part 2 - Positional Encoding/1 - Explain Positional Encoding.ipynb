{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain Positional Encoding\n",
    "\n",
    "In the previous two notebooks we got to know to important jargon related to training\\\n",
    "language models. We got to know embedding, vocabs, masking, ...\n",
    "\n",
    "In this notebook we will got to know \"positional encoding\". Early language models used\\\n",
    "Recurrent Neural Networks (RNNs) as backbones. In such models tokens are fed one after\\\n",
    "one in sequential manner. The dependency of one word over the previouse word was implicit.\n",
    "\n",
    "In transformers however as we saw in the masking notebook. We fed a sequence of words/tokens\\\n",
    "all at once. we know that words positions matters. The \"Why transformers are better than LSTM\"\\\n",
    "is different from \"Why LSTM are better than transformers\". The order matters.\n",
    "\n",
    "To fix the transformers missing positional information the authors of trasnformers deliberately\\\n",
    "added positional information. consider this:\n",
    "\n",
    "[\"Being\", \"strong\", \"is\", \"all\", \"what\", \"matters\"]\n",
    "\n",
    "To add the positional information we simply add the order. Like this.\n",
    "\n",
    "[1, 2, 3, 4, 5, 6]\n",
    "\n",
    "Wrong!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position as vectors\n",
    "\n",
    "Neural networks operates on real valued vectors. We can't simply add poistion as an integer\\\n",
    "in the input. For back-propagation to work and for higher layers ingesting positional information,\\\n",
    "position should be represented as a constant vector alongside the embedding.\n",
    "\n",
    "In mathmatical terms each token now is represented as following:\n",
    "$$ token = P_{token} + E_{token}$$\n",
    "\n",
    "Where the dimension of each vector P (position), E (Embedding) are of equal sizes.\\\n",
    "In exact representation of $E_{token}$ is $(e_0, e_1, e_2)$, $P_{token}$ is $(p_0, p_1, p_2)$\\\n",
    "It is important to understand that $P$ vector is constant among all sentences.\\\n",
    "We compute it once during model initialization and we use it with each entry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Positional Encoding\n",
    "So we need a mathmatical trick to create for each position in a sequence a vector\\\n",
    "of a sepcific size and certainly the vectors shouldn't be similar for a given position.\\\n",
    "The following part is not important you may skip it.\n",
    "\n",
    "The authors of the transformers introduced the following mathmatical trick to generate\\\n",
    "the positional vectors for input sequence:\n",
    "\n",
    "\\begin{align}\n",
    "  PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}) \\\\\n",
    "  PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})\n",
    "\\end{align}\n",
    "\n",
    "How intimidating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are transforming each position into a fixed vector. Assume that we set the embedding\\\n",
    "and positional encoding dimensionality into 30 and our sequence size is 10. Then each\\\n",
    "position from 1 to 10 now is represented as following:\n",
    "\n",
    "    1 ->  [a0, a1, a2, a3, a4, a5, a6, ..., a30]\n",
    "    2 ->  [b0, b1, b2, b3, b4, b5, b6, ..., b30]\n",
    "    .\n",
    "    10 -> [c0, c1, c2, c3, c4, c5, c6, ..., c30]\n",
    "\n",
    "The values a0-a30 and b0-b30 are computed as following:\n",
    "\n",
    "    a0 = sin( \"1\"/10000**(2\"0\"/\"30\") ) (even index 0 use sin)\n",
    "    a1 = cos( \"1\"/10000**(2\"1\"/\"30\") ) (odd index 1 use cos)\n",
    "    a2 = sin( \"1\"/10000**(2\"2\"/\"30\") ) (even index 2 use sin)\n",
    "    ...\n",
    "\n",
    "The application of these trignometric functions gurantees differnet vectors\\\n",
    "for each position. No matter the dimension size. Ensuring the values of all\\\n",
    "dimensions are less than one.\n",
    "In the below code we have a simple implemenation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8414709848078965, 0.9999976792064809, 4.641588833596115e-06]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def positional_encoding(pos, i, dim):\n",
    "    if i%2==0:\n",
    "        return np.sin(pos/10000**(2*i/dim))\n",
    "    else:\n",
    "        return np.cos(pos/10000**(2*i/dim))\n",
    "\n",
    "dim = 3\n",
    "pos_1_vec = [positional_encoding(1, i, dim) for i in range(dim)]\n",
    "pos_1_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice we can simply use pytorch's ``nn.embedding``."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

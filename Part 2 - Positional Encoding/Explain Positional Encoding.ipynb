{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain Positional Encoding\n",
    "\n",
    "In the previous two notebooks we got to know to important jargon realted to training\\\n",
    "language models. We got to know embedding, vocabs, masking, ...\n",
    "\n",
    "In this notebook we will got to know \"positional encoding\". Early language models used\\\n",
    "Recurrent Neural Networks (RNNs) as backbones. In such models tokens are fed one after\\\n",
    "one in sequential manner. The dependency of one word over the previouse word was implicit.\n",
    "\n",
    "In transformers however as we saw in the masking notebook. We fed a sequence of words/tokens\\\n",
    "all at once. we know that words positions matters. The \"Why transformers are better than LSTM\"\\\n",
    "is different from \"Why LSTM are better than transformers\". The order matters.\n",
    "\n",
    "To fix the transformers missing positional information the authors of trasnformers deliberately\\\n",
    "added positional information. consider this:\n",
    "\n",
    "[\"Being\", \"strong\", \"is\", \"all\", \"what\", \"matters\"]\n",
    "\n",
    "To add the positional information we simply add the order. Like this.\n",
    "\n",
    "[1, 2, 3, 4, 5, 6]\n",
    "\n",
    "Wrong!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position as vectors\n",
    "\n",
    "Neural networks operates on real valued vectors. We can't simply add poistion as an integer\\\n",
    "in the input. For back-propagation to work and for higher layers ingesting positional information,\\\n",
    "position should be represented as a constant vector alongside the embedding.\n",
    "\n",
    "In mathmatical terms each token now is represented as following:\n",
    "$$ token = P_{token} + E_{token}$$\n",
    "\n",
    "Where the dimension of each vector P (position), E (Embedding) are of equal sizes.\\\n",
    "In exact representation of $E_{token}$ is $(e_0, e_1, e_2)$, $P_{token}$ is $(p_0, p_1, p_2)$\\\n",
    "It is important to understand that $P$ vector is constant among all sentences.\\\n",
    "We compute it once during model initialization and we use it with each entry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Positional Encoding\n",
    "So we need a mathmatical trick to create for each position in a sequence a vector\\\n",
    "of a sepcific size and certainly the vectors shouldn't be similar for a given position.\\\n",
    "The following part is not important you may skip it.\n",
    "\n",
    "The authors of the transformers introduced the following mathmatical trick to generate\\\n",
    "the positional vectors for input sequence:\n",
    "\n",
    "$$\n",
    "PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}})\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})\n",
    "$$\n",
    "\n",
    "How intimidating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neural netowrks operates on c"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

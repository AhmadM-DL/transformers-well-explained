{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain Attention\n",
    "\n",
    "In the previous notebooks we familiries ourselves with  tokens, embeddings, vocabulary, masking, and positional encoding. In this notebook we will get to know \"Attention\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention is Abstraction\n",
    "The following is very important: From an application perspective, it allows higher layers in the architecture to operate on relations, grammar, and semantics rather than on raw words. Like how convolution allows the higher layers to operate on visual concepts rather than on raw pixels. Language is understood in a context. Words by themselves are symbols they get meaning when they are grouped. That is why we can take a paragraph and say \"This paragraph talks about immigration\" even if the word is not mentioned in the paragraph. This is the role of \"attention\" I would rather call it \"abstraction\" but well I didn't coin the term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Simple Math Behind Attention\n",
    "In previous articles, we see how words can be expressed as vectors in an n-dimensional space. But how can I express a sentence? A sentence is more than a collection of words. It includes grammar, time, action, and meaning. A sentence is a collection of words related to each other. If words are vectors in a space, then a sentence is the similarity matrix between those words. How to compute the similarity or \"distance\" between two vectors. By computing their dot product.\n",
    "Consider the sentence S=\"The cat is on the mat\". The visual concepts that pop into our minds are based on the sentence as a whole and how different parts relate to each other. Now each one of these words is represented as a vector that holds information about the word and its position (Remember we added positional encoding).\n",
    "\n",
    "Let V be the raw  representation of the sentence S. V is a matrix of size (n,d). Where n is the number of tokens 6. And d the dimensionality of the word embedding. V is a collection of words that don't contain the interaction of words in it. To get the attended representation of S, we multiply V itself with the interaction between words or the similarity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "V_{attended} = (V.V^T)V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V shape: (6, 10)\n",
      "VVT shape: (6, 6)\n",
      "V attended shape: (6, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# S = [\"Being\", \"Strong\", \"Is\", \"All\", \"What\", \"Matters\"]\n",
    "# (n) tokens = 6\n",
    "# (d) embedding dim = 10 \n",
    "# Each token is represented by a vector of 10 values \n",
    "\n",
    "V = np.random.rand(6, 10)\n",
    "print(f\"V shape: {V.shape}\")\n",
    "\n",
    "VVT = np.dot(V, V.T)\n",
    "print(f\"VVT shape: {VVT.shape}\")\n",
    "\n",
    "Vattended = np.dot(VVT, V)\n",
    "print(f\"V attended shape: {Vattended.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainable Attention[s]\n",
    "\n",
    "Now the attended matrix that we get assumes the representation or the word embedding is perfect. The truth is the word embedding isn't we are training it to be perfect. Similarly, the attention must be trained. You see we will end eventually by multiple attentions because each attention learns something about the sentence. Some attentions learn grammatical rules, others learn semantical rules. we have a lot of rules. \n",
    "\n",
    "For some rules, the words should be represented differently. Some words should get priority. Other words should be dimmed. And so on. Hence  for attention, we introduce three weights W1, W2, and W3. We multiply the weights with the Vs to get V1, V2, and V3 to get the trainable attention. \n",
    "\n",
    "You see by adding weights and using multiple attentions (Multihead Attention) we allow our model to learn different sets of rules. By adding more attention layers, we allow our model to even learn more and more complex rules.\n",
    "\n",
    "Note the V1, V2, and V3 happens to be called Q, K, and V.\\\n",
    "Also in practice we scale the similarity matrix QK^T and we apply softmax.\n",
    "\n",
    "$$\n",
    "\n",
    "A = Softmax(\\dfrac{Q.K^T}{\\sqrt{d_k}})V\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, sequence_size, input_dim, output_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.sequence_size = sequence_size\n",
    "        self.input_dim = input_dim\n",
    "        self.wq = nn.Parameter(torch.rand(input_dim, output_dim, requires_grad=True))\n",
    "        self.wk = nn.Parameter(torch.rand(input_dim, output_dim, requires_grad=True))\n",
    "        self.wv = nn.Parameter(torch.rand(input_dim, output_dim, requires_grad=True))\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        K = torch.matmul(x, self.wk)\n",
    "        Q = torch.matmul(x, self.wq)\n",
    "        V = torch.matmul(x, self.wv)\n",
    "\n",
    "        similarity_matrix = torch.matmul(Q, K.transpose(1, 2)) / torch.sqrt(torch.as_tensor(self.input_dim))\n",
    "        similarity_matrix = self.softmax(similarity_matrix)\n",
    "\n",
    "        return torch.matmul(similarity_matrix, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (6, 10)\n",
      "y shape: torch.Size([3, 6, 32])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# S = [\"Being\", \"Strong\", \"Is\", \"All\", \"What\", \"Matters\"]\n",
    "# (n) tokens = 6\n",
    "# (d) embedding dim = 10 \n",
    "# Each token is represented by a vector of 10 values \n",
    "\n",
    "x = torch.rand(3, 6, 10)\n",
    "print(f\"x shape: {V.shape}\")\n",
    "\n",
    "attention = Attention(6, 10, 32)\n",
    "y = attention(x)\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

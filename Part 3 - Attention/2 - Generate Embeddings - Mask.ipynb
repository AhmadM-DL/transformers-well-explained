{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dislaimer\n",
    "This notebook requires knowledge in:\n",
    "* Python\n",
    "* Neural Networks\n",
    "* Pytorch Datasets and Modules\n",
    "* Machine Learning Process Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings Example\n",
    "The goal of this notebook is to have a hands-on experience of words embeddings.\\\n",
    "We will do the following:\n",
    "* Load a set of Arabic text as trigrams\n",
    "* Build a simple neural network\n",
    "* Train the network to **predict masked words**\n",
    "* Use both word embeddings and positional embeddings and attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Dataset\n",
    "\n",
    "To train an embedding, we will use the dataset we used in the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch import as_tensor\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json, re, math\n",
    "import numpy as np\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, alaraby_filepath, max_length, is_train):\n",
    "        self.raw_data = [article[\"text\"] for article in json.load(open(alaraby_filepath, \"r\"))]\n",
    "        self.train_raw_data, self.test_raw_data = train_test_split(self.raw_data, test_size=0.1, random_state=42, shuffle=False)\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.SPECIAL_TOKENS_IDS = {\"<UNKOWN>\": 0, \"<PAD>\": 1, \"<MASK>\": 2}\n",
    "        self.SPECIAL_IDS_TOKENS = {0: \"<UNKOWN>\", 1: \"<PAD>\", 2: \"<MASK>\"}\n",
    "\n",
    "        self.train_sequences =  self.__generate_seq__(self.train_raw_data, self.max_length)\n",
    "        self.test_sequences =  self.__generate_seq__(self.test_raw_data, self.max_length)\n",
    "        self.vocab, self.id_to_word, self.word_to_id = self.__compute_vocab__(self.train_raw_data)\n",
    "\n",
    "\n",
    "        self.is_train = is_train\n",
    "    \n",
    "    def __generate_seq__(self, texts, max_length):\n",
    "        sentences = [re.split('\\.|،|!|؟', text) for text in texts]\n",
    "        sequences = [s for sequences in sentences for s in sequences]\n",
    "        sequences = [s.split() for s in sequences]\n",
    "        sequences = [s for s in sequences if len(s) <= max_length and len(s)>0]\n",
    "        return sequences\n",
    "    \n",
    "    def __compute_vocab__(self, texts):\n",
    "        words = {}\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                words[word]= None\n",
    "        words_list = list(self.SPECIAL_TOKENS_IDS.keys()) + list(words)\n",
    "        id_to_word = defaultdict(lambda: \"<UNKNOWN>\", {idx: value for idx, value in enumerate(words_list)})\n",
    "        word_to_id = defaultdict(lambda: 0, {value: idx for idx, value in enumerate(words_list)})\n",
    "        return words, id_to_word, word_to_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_sequences) if self.is_train else len(self.test_sequences)\n",
    "\n",
    "    def __getitem__(self, idx, mask_prob=15):\n",
    "        mask_prob = 0.15\n",
    "        sequence = self.train_sequences[idx]\n",
    "\n",
    "        masked_indicies = np.random.choice(len(sequence), math.ceil(len(sequence)*mask_prob))\n",
    "        mask = [i in masked_indicies for i,_ in enumerate(sequence)]\n",
    "        masked_sequence = list(np.where(mask, \"<MASK>\", sequence))\n",
    "        masked_sequence =  masked_sequence + [\"<PAD>\"]*(self.max_length - len(masked_sequence))\n",
    "        masked_sequence = [ self.word_to_id[token] for token in masked_sequence]\n",
    "\n",
    "        target = list(np.where(np.invert(mask), \"<PAD>\", sequence))\n",
    "        target = [\"<PAD>\"]*(self.max_length - len(target)) + target\n",
    "        target = [ self.word_to_id[token] for token in target]\n",
    "\n",
    "        return as_tensor(masked_sequence), as_tensor(target)\n",
    "    \n",
    "    def get_word_from_id(self, idx):\n",
    "        return self.id_to_word[idx]\n",
    "    \n",
    "    def get_word_id(self, word):\n",
    "        return self.word_to_id[word]\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 10\n",
    "train_dataset = MyDataset(\"../Dataset/alaraby1k.json\", max_length, is_train= True)\n",
    "test_dataset = MyDataset(\"../Dataset/alaraby1k.json\", max_length, is_train= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Id of unkown word: {train_dataset.get_word_id('<UNKNOWN>')}\")\n",
    "print(f\"ًWord of unkown id: {train_dataset.get_word_from_id(-1)}\")\n",
    "print(f\"Vocab size: {train_dataset.get_vocab_size()}\")\n",
    "\n",
    "item = 3100\n",
    "seq, target = train_dataset.__getitem__(item)\n",
    "print(f\"\\nItem {item} seq: \")\n",
    "for word in [train_dataset.get_word_from_id(t.item()) for t in seq]:\n",
    "  print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Neural Network\n",
    "\n",
    "We will use the same network we used in previous notebooks. However, this\\\n",
    "time we add a new layer ``Attention``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, sequence_size, input_dim, output_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.sequence_size = sequence_size\n",
    "        self.input_dim = input_dim\n",
    "        self.wq = torch.rand(input_dim, output_dim, requires_grad=True)\n",
    "        self.wk = torch.rand(input_dim, output_dim, requires_grad=True)\n",
    "        self.wv = torch.rand(input_dim, output_dim, requires_grad=True)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        K = torch.matmul(x, self.wk)\n",
    "        Q = torch.matmul(x, self.wq)\n",
    "        V = torch.matmul(x, self.wv)\n",
    "\n",
    "        similarity_matrix = torch.matmul(Q, K.transpose(1, 2)) / torch.sqrt(torch.as_tensor(self.input_dim))\n",
    "        similarity_matrix = self.softmax(similarity_matrix)\n",
    "\n",
    "        return torch.matmul(similarity_matrix, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MaskedWordPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_length):\n",
    "        super(MaskedWordPredictor, self).__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)    \n",
    "        self.linear = nn.Linear(max_length * embedding_dim, max_length * vocab_size)\n",
    "        self.attention = Attention(max_length, embedding_dim, embedding_dim)\n",
    "\n",
    "        self.positional_embedding = self.__generate_positional_embedding__(max_length, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size= x.size()[0]\n",
    "        output = self.embedding(x) # (batch_size, max_length, embedding_size)\n",
    "        output = output + self.positional_embedding.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        output = self.attention(output) # (batch_size, max_length, embedding_size)\n",
    "        output = output.reshape(batch_size, -1) # (batch_size, max_length * embedding_size)\n",
    "        output = self.linear(output) # (batch_size, max_length * vocab_size)\n",
    "        output = output.reshape(batch_size, self.max_length, self.vocab_size) # (batch_size, max_length, vocab_size)\n",
    "        return output\n",
    "\n",
    "    def embed(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "    def word(self, x):\n",
    "      distance = torch.norm(self.embedding.weight.data - x, dim=1)\n",
    "      nearest = torch.argmin(distance)\n",
    "      return nearest\n",
    "    \n",
    "    def __generate_positional_embedding__(self, max_length, embedding_dim):\n",
    "        layer = nn.Embedding(max_length, embedding_dim)\n",
    "        pos_seq = torch.arange(start=0, end=max_length, requires_grad=False)\n",
    "        embedding = layer(pos_seq).detach()\n",
    "        return embedding\n",
    "    \n",
    "    def to(self, device):\n",
    "      super(MaskedWordPredictor, self).to(device)\n",
    "      self.positional_embedding = self.positional_embedding.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Embedding\n",
    "\n",
    "In this code chunk we simply train the network given a number of hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" ) if torch.cuda.is_available() else torch.device(\"cpu\" )\n",
    "\n",
    "batch_size= 10\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "vocab_size = 60_000\n",
    "embbeding_dim = 128\n",
    "model = MaskedWordPredictor(vocab_size, embbeding_dim, max_length)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "chkpnt_path = f\"masked_word_predictor_{vocab_size}_{embbeding_dim}.chk\"\n",
    "if os.path.exists(chkpnt_path):\n",
    "  model.load_state_dict(torch.load(chkpnt_path, map_location=device))\n",
    "\n",
    "\n",
    "progress_path = \"/masked_word_predictor_progress.json\"\n",
    "if os.path.exists(progress_path):\n",
    "  progress = json.load(open(progress_path, \"r\"))\n",
    "else:\n",
    "  progress = {\"chkpnt\" : 0, \"progress\" : []}\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(progress[\"chkpnt\"], epochs):\n",
    "  for i, batch in enumerate(train_dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    seq, target = batch\n",
    "    seq = seq.to(device)\n",
    "    target = target.to(device)\n",
    "    output = model(seq).transpose(1, 2) # cross entropy loss expects shape [batches, nb_classes, dim1]\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if(i % 10==0):\n",
    "      print(f\"Epoch {epoch } Batch {i}, Loss: {loss.item()}\")\n",
    "  progress[\"progress\"].append({\"epoch\": epoch, \"loss\": loss.item()})\n",
    "  progress[\"chkpnt\"] = epoch   \n",
    "  json.dump(progress, open(progress_path, \"w\"))\n",
    "  torch.save(model.state_dict(), chkpnt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "chkpnt = \"/content/masked_word_predictor_60000_128.chk\"\n",
    "device = torch.device(\"cuda:0\" ) if torch.cuda.is_available() else torch.device(\"cpu\" )\n",
    "\n",
    "max_length = 10\n",
    "\n",
    "mask = 4\n",
    "seq = np.array([\"هل\", \"فرش\",  \"+ت\", \"ال+\", \"<MASK>\", \"يوما\"])\n",
    "seq = np.pad(seq, (0,max_length - len(seq)))\n",
    "seq = [train_dataset.word_to_id[w] for w in seq]\n",
    "seq = torch.as_tensor(seq).view(1, -1).to(device)\n",
    "\n",
    "vocab_size = 60_000\n",
    "embbeding_dim = 128\n",
    "inference = MaskedWordPredictor(vocab_size, embbeding_dim, max_length).to(device)\n",
    "inference.load_state_dict(torch.load(chkpnt, map_location=device))\n",
    "\n",
    "output = inference(seq)\n",
    "top_3 = np.argsort(output.detach().cpu().numpy(), axis=2)[:, :, -3:]\n",
    "top_3 = top_3[0, 4, :]\n",
    "\n",
    "print(f\"Word 1: {train_dataset.id_to_word[top_3[0]]}\")\n",
    "print(f\"Word 2: {train_dataset.id_to_word[top_3[1]]}\")\n",
    "print(f\"Word 3: {train_dataset.id_to_word[top_3[2]]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

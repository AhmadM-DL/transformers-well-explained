# transformers-well-explained

TL;DR A collection of notebooks to explain Transformers (The right way)

A series of concise articles explaining Transformers and the attention mechanism in a way that truly makes sense. I'll be referring to the landmark paper "[Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)" and breaking down the main aspects of Transformers, focusing on one concept per article. Each concept will be supported by a notebook for a hands-on understanding. The concepts discussed are:

* Word-Embeddings
* Positional-Encoding
* Self-Attention
* Cross-Attention

The notebooks are supposed to me a suppurtive material for the articles on medium.


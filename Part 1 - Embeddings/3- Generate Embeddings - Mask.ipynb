{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dislaimer\n",
    "This notebook requires knowledge in:\n",
    "* Python\n",
    "* Neural Networks\n",
    "* Pytorch Datasets and Modules\n",
    "* Machine Learning Process Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings Example\n",
    "The goal of this notebook is to have a hands-on experience of words embeddings.\\\n",
    "We will do the following:\n",
    "* Load a set of Arabic text as trigrams\n",
    "* Build a simple neural network\n",
    "* Train the network to **predict masked words**\n",
    "* Use the network weights as embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Dataset\n",
    "\n",
    "To train an embedding, we will use the dataset we used in the previous notebook\n",
    "\n",
    "In the previous notebook we added the token `<UNKOWN>` to be a surregate for any unkown word\\\n",
    "that might be entered to the model during testing or deployment. For training by masking we will\\\n",
    "add two more tokens `<PAD>` and `<MASK>`.\n",
    "\n",
    "Unlike in the next word prediction task, masked word prediction\\\n",
    "task take a sequence of tokens of a specific length `max_length`. Some squences will be shorter\\\n",
    "than the maximum length, hence the usage of `<PAD>` to fill in the remaining slots in the\\\n",
    "input sequence. Some of these tokens will be masked and the the model will have to predict\\\n",
    "them, hence the usage of `<MASK>`.\n",
    "\n",
    "During the dataset initlization we will load the entire dataset in `alaraby1k.json` and then\\\n",
    "generate sequnces based on sentences in the data set. We will split the text by `[! ? ، .]`\\\n",
    "and consider each sentence a sequence. If the sequence is longer than `max_length` we wil discard it.\\\n",
    "We set `max_length` to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json, re\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, alaraby_filepath, max_length, is_train):\n",
    "        self.raw_data = [article[\"text\"] for article in json.load(open(alaraby_filepath, \"r\"))]\n",
    "        self.train_raw_data, self.test_raw_data = train_test_split(self.raw_data, test_size=0.1, random_state=42)\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.SPECIAL_TOKENS_IDS = {\"<UNKOWN>\": 0, \"<PAD>\": 1, \"<MASK>\": 2}\n",
    "        self.SPECIAL_IDS_TOKENS = {0: \"<UNKOWN>\", 1: \"<PAD>\", 2: \"<MASK>\"}\n",
    "\n",
    "        self.train_sequences =  self.__generate_seq__(self.train_raw_data, self.max_length)\n",
    "        self.test_sequences =  self.__generate_seq__(self.test_raw_data, self.max_length)\n",
    "        self.vocab, self.id_to_word, self.word_to_id = self.__compute_vocab__(self.train_raw_data)\n",
    "\n",
    "\n",
    "        self.is_train = is_train\n",
    "    \n",
    "    def __generate_seq__(self, texts, max_length):\n",
    "        sentences = [re.split('\\.|،|!|؟', text) for text in texts]\n",
    "        sequences = [s for sequences in sentences for s in sequences]\n",
    "        sequences = [s.split() for s in sequences]\n",
    "        sequences = [s for s in sequences if len(s) <= max_length]\n",
    "        return sequences\n",
    "    \n",
    "    def __compute_vocab__(self, texts):\n",
    "        words = set()\n",
    "        for text in texts:\n",
    "            words.update(set(text.split()))\n",
    "        words_list = list(self.SPECIAL_TOKENS_IDS.keys()) + list(words)\n",
    "        id_to_word = defaultdict(lambda: \"<UNKNOWN>\", {idx: value for idx, value in enumerate(words_list)})\n",
    "        word_to_id = defaultdict(lambda: 0, {value: idx for idx, value in enumerate(words_list)})\n",
    "        return words, id_to_word, word_to_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_sequences) if self.is_train else len(self.test_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequences = self.train_sequences if self.is_train else self.test_sequences\n",
    "        sequence = sequences[idx]\n",
    "        sequence = [\"<PAD>\"]*(self.max_length - len(sequence)) + sequence\n",
    "        sequence = [ self.word_to_id[token] for token in sequence]\n",
    "        return sequence\n",
    "    \n",
    "    def get_word_from_id(self, idx):\n",
    "        return self.id_to_word[idx]\n",
    "    \n",
    "    def get_word_id(self, word):\n",
    "        return self.word_to_id[word]\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "train_dataset = MyDataset(\"../Dataset/alaraby1k.json\", max_length, is_train= True)\n",
    "test_dataset = MyDataset(\"../Dataset/alaraby1k.json\", max_length, is_train= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id of unkown word: 0\n",
      "ًWord of unkown id: <UNKNOWN>\n",
      "Vocab size: 163101\n",
      "Item 0 in ids: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 71314, 116719, 106632, 157858, 50174, 31002, 9641, 49873, 154270, 136311, 48266, 16153, 17301, 106632, 95204, 52240, 21155, 118809, 44523, 101061, 76844, 145301, 95617, 153465, 30667, 106632, 69867, 23890, 78859, 150919, 69612, 24505, 69054, 18047, 374]\n",
      "Item 0 in words: [['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'إيفو', 'أندريتش', 'من', 'الروائيين', 'القلائل', 'الذين', 'حصلوا', 'على', 'جائزة', 'نوبل', 'عن', 'جدارة', 'وهو', 'من', 'أبناء', 'بوسنة', 'وهرسك', 'أو', 'كما', 'ينطقونها', 'بوزنو', 'وهرتسجوفينو', 'عاش', 'في', 'ثلاث', 'من', 'مدنها', 'هي', 'ترافنيك', 'التي', 'ولد', 'فيها', 'وكتب', 'رواية', 'عنها']\n",
      "Item 33 in ids: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 59257, 112910, 2562, 138375, 76518, 5508, 29135, 118912]\n",
      "Item 33 in words: [['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'بعد', 'أن', 'احتلته', 'قوات', 'ألمانيا', 'والمجر', 'وإيطاليا', 'وبلغاريا']\n",
      "Item 1200 in ids: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 90640, 37615, 30439, 153465, 91689, 136174, 81553, 3832]\n",
      "Item 1200 in words: [['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'فقد', 'بقيت', 'السيطرة', 'في', 'المجتمع', 'الصناعي', 'للعمل', 'اليدوي']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Id of unkown word: {train_dataset.get_word_id('<UNKNOWN>')}\")\n",
    "print(f\"ًWord of unkown id: {train_dataset.get_word_from_id(-1)}\")\n",
    "print(f\"Vocab size: {train_dataset.get_vocab_size()}\")\n",
    "\n",
    "item = 0\n",
    "seq = train_dataset.__getitem__(item)\n",
    "print(f\"Item {item} in ids: {str(seq)}\")\n",
    "print(f\"Item {item} in words: [{str([train_dataset.get_word_from_id(t) for t in seq])}\")\n",
    "\n",
    "item = 33\n",
    "seq = train_dataset.__getitem__(item)\n",
    "print(f\"Item {item} in ids: {str(seq)}\")\n",
    "print(f\"Item {item} in words: [{str([train_dataset.get_word_from_id(t) for t in seq])}\")\n",
    "\n",
    "item = 1200\n",
    "seq = train_dataset.__getitem__(item)\n",
    "print(f\"Item {item} in ids: {str(seq)}\")\n",
    "print(f\"Item {item} in words: [{str([train_dataset.get_word_from_id(t) for t in seq])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Neural Network\n",
    "\n",
    "The network task is framed as a straightforward classification task. Consider the `max_length`\\\n",
    "of the input sequence. Consider the `vocab_size`. The input is `max_length x vocab_size`.\\\n",
    "We are predicting masked words so the output has the same size as the input.\\\n",
    "We need to predict the same sequence arn't we. The logits size will be\\\n",
    "also `max_length x vocab_size` then. We will then compute the `cross_entropy` loss.\n",
    "\n",
    "However, asking the model to predict the same output except for some masked words is reduntand.\\\n",
    "We will be computing loss over output tokens that we already know. To solve the problem \\\n",
    "we will do the following. We will compute the `cross_entropy` loss with a an altered verions\\\n",
    "of the input. Consider the following:\n",
    "\n",
    "$$\n",
    "The \\ input: \n",
    "\\begin{bmatrix}\n",
    "    1 , 1 , 1 , 1 (word 1)\\\\ \n",
    "    1 , 1 , 1 , 1 (.) \\\\\n",
    "    1 , 1 , 1 , 1 (.) \\\\\n",
    "    . , . , . , . (.) \\\\\n",
    "    1 , 1 , 1 , 1 (word 2) \\\\ \n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "We will be using `nn.embedding` layer from pytroch. Internally the layer is a weight matrix of\\\n",
    "the following size `(embedding size, vocab size)`. The `embedding size` it the size of vecotr\\\n",
    "we want to represent the words with it is arbitrary chosen and optimized using hyperparameter\\\n",
    "optimization. The `vocab size` is the size of the vocab or the number of words we have.\\\n",
    "The matrix is updated and optimized using back propagation. We\n",
    "\n",
    "Our vocab size is 163101 we will use an embedding of vocab size 165,000. And we will\\\n",
    "choose the embedding size to be `1024`. Which means each word will be represented using\\\n",
    "an a vector of size `1024`. Given the complex nature of languages in general \\\n",
    "an given the hard nature of the given task, one should use a large embedding size\\\n",
    "this gives the model more capacity to learn more and more about the semantics of the data.\n",
    "\n",
    "Finally, be informed that `nn.embedding` is pretty similar to `nn.linear` layer.\\\n",
    "With a main difference that embedding takes words indices in the vocabulary\\\n",
    "while the linear layer takes one-hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class WordPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(WordPredictor, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim*2, vocab_size)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        embedded1 = self.embedding(x1)\n",
    "        embedded2 = self.embedding(x2)\n",
    "        concatenated = torch.cat((embedded1, embedded2), dim=1)\n",
    "        output = self.linear(concatenated)\n",
    "        return output\n",
    "\n",
    "    def embed(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "    def word(self, x):\n",
    "      distance = torch.norm(self.embedding.weight.data - x, dim=1)\n",
    "      nearest = torch.argmin(distance)\n",
    "      return nearest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Embedding\n",
    "\n",
    "In this code chunk we simply train the network given a number of hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" ) if torch.cuda.is_available() else torch.device(\"cpu\" )\n",
    "\n",
    "batch_size= 1500\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "vocab_size = 165_000\n",
    "embbeding_dim = 1024\n",
    "model = WordPredictor(vocab_size, embbeding_dim)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "chkpnt_path = f\"word_predictor_{vocab_size}_{embbeding_dim}.chk\"\n",
    "if os.path.exists(chkpnt_path):\n",
    "  model.load_state_dict(torch.load(chkpnt_path, map_location=device))\n",
    "\n",
    "\n",
    "progress_path = \"/word_predictor_progress.json\"\n",
    "if os.path.exists(progress_path):\n",
    "  progress = json.load(open(progress_path, \"r\"))\n",
    "else:\n",
    "  progress = {\"chkpnt\" : 0, \"progress\" : []}\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in (progress[\"chkpnt\"], epochs):\n",
    "  for i, batch in enumerate(train_dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    x1, x2, target = batch\n",
    "    x1 = x1.to(device)\n",
    "    x2 = x2.to(device)\n",
    "    target = target.to(device)\n",
    "    output = model(x1, x2)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if(i % 10==0):\n",
    "      print(f\"Epoch {epoch } Batch {i}, Loss: {loss.item()}\")\n",
    "  progress[\"progress\"].append({\"epoch\": epoch, \"loss\": loss.item()})\n",
    "  progress[\"chkpnt\"] = epoch   \n",
    "  json.dump(progress, open(progress_path, \"w\"))\n",
    "  torch.save(model.state_dict(), chkpnt_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

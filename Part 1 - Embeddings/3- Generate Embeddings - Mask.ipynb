{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dislaimer\n",
    "This notebook requires knowledge in:\n",
    "* Python\n",
    "* Neural Networks\n",
    "* Pytorch Datasets and Modules\n",
    "* Machine Learning Process Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings Example\n",
    "The goal of this notebook is to have a hands-on experience of words embeddings.\\\n",
    "We will do the following:\n",
    "* Load a set of Arabic text as trigrams\n",
    "* Build a simple neural network\n",
    "* Train the network to **predict masked words**\n",
    "* Use the network weights as embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Dataset\n",
    "\n",
    "To train an embedding, we will use the dataset we used in the previous notebook\n",
    "\n",
    "In the previous notebook we added the token `<UNKOWN>` to be a surregate for any unkown word\\\n",
    "that might be entered to the model during testing or deployment. For training by masking we will\\\n",
    "add two more tokens `<PAD>` and `<MASK>`.\n",
    "\n",
    "Unlike in the next word prediction task, masked word prediction\\\n",
    "task take a sequence of tokens of a specific length `max_length`. Some squences will be shorter\\\n",
    "than the maximum length, hence the usage of `<PAD>` to fill in the remaining slots in the\\\n",
    "input sequence. Some of these tokens will be masked and the the model will have to predict\\\n",
    "them, hence the usage of `<MASK>`.\n",
    "\n",
    "During the dataset initlization we will load the entire dataset in `alaraby1k.json` and then\\\n",
    "generate sequnces based on sentences in the data set. We will split the text by `[! ? ، .]`\\\n",
    "and consider each sentence a sequence. If the sequence is longer than `max_length` we wil discard it.\\\n",
    "We set `max_length` to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch import as_tensor\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json, re, math\n",
    "import numpy as np\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, alaraby_filepath, max_length, is_train):\n",
    "        self.raw_data = [article[\"text\"] for article in json.load(open(alaraby_filepath, \"r\"))]\n",
    "        self.train_raw_data, self.test_raw_data = train_test_split(self.raw_data, test_size=0.1, random_state=42, shuffle=False)\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.SPECIAL_TOKENS_IDS = {\"<UNKOWN>\": 0, \"<PAD>\": 1, \"<MASK>\": 2}\n",
    "        self.SPECIAL_IDS_TOKENS = {0: \"<UNKOWN>\", 1: \"<PAD>\", 2: \"<MASK>\"}\n",
    "\n",
    "        self.train_sequences =  self.__generate_seq__(self.train_raw_data, self.max_length)\n",
    "        self.test_sequences =  self.__generate_seq__(self.test_raw_data, self.max_length)\n",
    "        self.vocab, self.id_to_word, self.word_to_id = self.__compute_vocab__(self.train_raw_data)\n",
    "\n",
    "\n",
    "        self.is_train = is_train\n",
    "    \n",
    "    def __generate_seq__(self, texts, max_length):\n",
    "        sentences = [re.split('\\.|،|!|؟', text) for text in texts]\n",
    "        sequences = [s for sequences in sentences for s in sequences]\n",
    "        sequences = [s.split() for s in sequences]\n",
    "        sequences = [s for s in sequences if len(s) <= max_length and len(s)>0]\n",
    "        return sequences\n",
    "    \n",
    "    def __compute_vocab__(self, texts):\n",
    "        words = {}\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                words[word]= None\n",
    "        words_list = list(self.SPECIAL_TOKENS_IDS.keys()) + list(words)\n",
    "        id_to_word = defaultdict(lambda: \"<UNKNOWN>\", {idx: value for idx, value in enumerate(words_list)})\n",
    "        word_to_id = defaultdict(lambda: 0, {value: idx for idx, value in enumerate(words_list)})\n",
    "        return words, id_to_word, word_to_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_sequences) if self.is_train else len(self.test_sequences)\n",
    "\n",
    "    def __getitem__(self, idx, mask_prob=15):\n",
    "        mask_prob = 0.15\n",
    "        sequence = self.train_sequences[idx]\n",
    "\n",
    "        masked_indicies = np.random.choice(len(sequence), math.ceil(len(sequence)*mask_prob))\n",
    "        mask = [i in masked_indicies for i,_ in enumerate(sequence)]\n",
    "        masked_sequence = list(np.where(mask, \"<MASK>\", sequence))\n",
    "        masked_sequence =  masked_sequence + [\"<PAD>\"]*(self.max_length - len(masked_sequence))\n",
    "        masked_sequence = [ self.word_to_id[token] for token in masked_sequence]\n",
    "\n",
    "        target = list(np.where(np.invert(mask), \"<PAD>\", sequence))\n",
    "        target = [\"<PAD>\"]*(self.max_length - len(target)) + target\n",
    "        target = [ self.word_to_id[token] for token in target]\n",
    "\n",
    "        return as_tensor(masked_sequence), as_tensor(target)\n",
    "    \n",
    "    def get_word_from_id(self, idx):\n",
    "        return self.id_to_word[idx]\n",
    "    \n",
    "    def get_word_id(self, word):\n",
    "        return self.word_to_id[word]\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 10\n",
    "train_dataset = MyDataset(\"../Dataset/alaraby1k.json\", max_length, is_train= True)\n",
    "test_dataset = MyDataset(\"../Dataset/alaraby1k.json\", max_length, is_train= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Id of unkown word: {train_dataset.get_word_id('<UNKNOWN>')}\")\n",
    "print(f\"ًWord of unkown id: {train_dataset.get_word_from_id(-1)}\")\n",
    "print(f\"Vocab size: {train_dataset.get_vocab_size()}\")\n",
    "\n",
    "item = 3100\n",
    "seq, target = train_dataset.__getitem__(item)\n",
    "print(f\"\\nItem {item} seq: \")\n",
    "for word in [train_dataset.get_word_from_id(t.item()) for t in seq]:\n",
    "  print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Neural Network\n",
    "\n",
    "The network task is framed as a straightforward classification task. Consider the `max_length`\\\n",
    "of the input sequence. The input size is `max_length x 1`. Where one correspondes to the token\\\n",
    "index. We are predicting masked words so the output has the same size as the input. We need to\\\n",
    "predict the same sequence arn't we. The logits size will be also `max_length x vocab_size` then.\\\n",
    "We will then compute pytorch `cross_entropy` loss. Which will map the logits into the classification\\\n",
    "target `max_length x 1`.\n",
    "\n",
    "However, asking the model to predict the same output except for some masked words is reduntand.\\\n",
    "We will be computing loss over output tokens that we already know. To solve the problem \\\n",
    "we will do the following. We will compute the `cross_entropy` loss with a an altered verions\\\n",
    "of the input. Consider the following example:\n",
    "\n",
    "\n",
    "Input  | Being | strong | is       | all | what      | matters | `<PAD>`\n",
    "--     |    -- |     -- |       -- |  -- |        -- |      -- | --\n",
    "Masked | Being | strong | `<MASK>` | all | `<MASK>`  | matters | `<PAD>`\n",
    "Embed  | 22    | 433    | 1        | 333 | 1         | 331     | 2\n",
    "Target | 2     | 2      | 190      | 2   | 355       | 2       | 2\n",
    "\n",
    "As you can see we mapped all non-masked words in the target vecotr into `<PAD>`.\\\n",
    "And we keep the class indices of masked words. We then ignore the class index of\\\n",
    "`<PAD>` when computing the loss. By doing this we compute the loss and optimize\\\n",
    "the model weights based only on the masked words predictions. We can achive this\\\n",
    "behaviour by using the `ignore-index` parameter of [Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) `CrossEntropyLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MaskedWordPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_length):\n",
    "        super(MaskedWordPredictor, self).__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(max_length * embedding_dim, max_length * vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x) # (batch_size, max_length, embedding_size)\n",
    "        embedded = embedded.reshape(x.size()[0], -1) # (batch_size, max_length * embedding_size)\n",
    "        output = self.linear(embedded) # (batch_size, max_length * vocab_size)\n",
    "        output = output.reshape(x.size()[0], self.max_length, self.vocab_size) # (batch_size, max_length, vocab_size)\n",
    "        return output\n",
    "\n",
    "    def embed(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "    def word(self, x):\n",
    "      distance = torch.norm(self.embedding.weight.data - x, dim=1)\n",
    "      nearest = torch.argmin(distance)\n",
    "      return nearest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Embedding\n",
    "\n",
    "In this code chunk we simply train the network given a number of hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" ) if torch.cuda.is_available() else torch.device(\"cpu\" )\n",
    "\n",
    "batch_size= 10\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "vocab_size = 60_000\n",
    "embbeding_dim = 128\n",
    "model = MaskedWordPredictor(vocab_size, embbeding_dim, max_length)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "chkpnt_path = f\"masked_word_predictor_{vocab_size}_{embbeding_dim}.chk\"\n",
    "if os.path.exists(chkpnt_path):\n",
    "  model.load_state_dict(torch.load(chkpnt_path, map_location=device))\n",
    "\n",
    "\n",
    "progress_path = \"/masked_word_predictor_progress.json\"\n",
    "if os.path.exists(progress_path):\n",
    "  progress = json.load(open(progress_path, \"r\"))\n",
    "else:\n",
    "  progress = {\"chkpnt\" : 0, \"progress\" : []}\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(progress[\"chkpnt\"], epochs):\n",
    "  for i, batch in enumerate(train_dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    seq, target = batch\n",
    "    seq = seq.to(device)\n",
    "    target = target.to(device)\n",
    "    output = model(seq).transpose(1, 2) # cross entropy loss expects shape [batches, nb_classes, dim1]\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if(i % 10==0):\n",
    "      print(f\"Epoch {epoch } Batch {i}, Loss: {loss.item()}\")\n",
    "  progress[\"progress\"].append({\"epoch\": epoch, \"loss\": loss.item()})\n",
    "  progress[\"chkpnt\"] = epoch   \n",
    "  json.dump(progress, open(progress_path, \"w\"))\n",
    "  torch.save(model.state_dict(), chkpnt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "chkpnt = \"/content/masked_word_predictor_60000_128.chk\"\n",
    "device = torch.device(\"cuda:0\" ) if torch.cuda.is_available() else torch.device(\"cpu\" )\n",
    "\n",
    "max_length = 10\n",
    "\n",
    "mask = 4\n",
    "seq = np.array([\"هل\", \"فرش\",  \"+ت\", \"ال+\", \"<MASK>\", \"يوما\"])\n",
    "seq = np.pad(seq, (0,max_length - len(seq)))\n",
    "seq = [train_dataset.word_to_id[w] for w in seq]\n",
    "seq = torch.as_tensor(seq).view(1, -1).to(device)\n",
    "\n",
    "vocab_size = 60_000\n",
    "embbeding_dim = 128\n",
    "inference = MaskedWordPredictor(vocab_size, embbeding_dim, max_length).to(device)\n",
    "inference.load_state_dict(torch.load(chkpnt, map_location=device))\n",
    "\n",
    "output = inference(seq)\n",
    "top_3 = np.argsort(output.detach().cpu().numpy(), axis=2)[:, :, -3:]\n",
    "top_3 = top_3[0, 4, :]\n",
    "\n",
    "print(f\"Word 1: {train_dataset.id_to_word[top_3[0]]}\")\n",
    "print(f\"Word 2: {train_dataset.id_to_word[top_3[1]]}\")\n",
    "print(f\"Word 3: {train_dataset.id_to_word[top_3[2]]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

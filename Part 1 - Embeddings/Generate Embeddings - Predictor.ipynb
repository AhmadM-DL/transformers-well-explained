{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dislaimer\n",
    "This notebook requires knowledge in:\n",
    "* Python\n",
    "* Neural Networks\n",
    "* Pytorch Datasets and Modules\n",
    "* Machine Learning Process Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings Example\n",
    "The goal of this notebook is to have a hands-on experience of words embeddings.\\\n",
    "We will do the following:\n",
    "* Load a set of Arabic text as trigrams\n",
    "* Build a simple neural network\n",
    "* Train the network over the dataset\n",
    "* Use the network weights as embeddings\n",
    "* Use tsne to plot a set of words and see the distances between words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Dataset\n",
    "We will use a simple dataset scraped from the al-araby magazine.\\\n",
    "It contains 1000 arabic article. The \"alarby1k.json\" file contains 1000 article entry.\\\n",
    "Each entry contains the ```author``` name the ```issue``` number and the article ```text```.\n",
    "\n",
    "The below code chunk loads the text of all the articles and generate all possible trigrams.\\\n",
    "We encapsulate the dataset with a customer pytorch ```Dataset``` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, alaraby_filepath, is_train):\n",
    "        self.raw_data = [article[\"text\"] for article in json.load(open(alaraby_filepath, \"r\"))]\n",
    "        self.train_raw_data, self.test_raw_data = train_test_split(self.raw_data, test_size=0.1, random_state=42)\n",
    "        self.train_trigrams =  self.__generate_trigrams__(self.train_raw_data)\n",
    "        self.test_trigrams =  self.__generate_trigrams__(self.test_raw_data)\n",
    "        self.vocab, self.id_to_word, self.word_to_id = self.__compute_vocab__(self.train_raw_data)\n",
    "        self.is_train = is_train\n",
    "    \n",
    "    def __generate_trigrams__(self, texts):\n",
    "        trigrams = []\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            article_trigrams = [words[i:i+3] for i in range(len(words)-2)]\n",
    "            trigrams+= article_trigrams\n",
    "        return trigrams\n",
    "    \n",
    "    def __compute_vocab__(self, texts):\n",
    "        # Get unique words\n",
    "        words = set()\n",
    "        for text in texts:\n",
    "            words.update(set(text.split()))\n",
    "        # Generate dictionary to get word from id and vice versa\n",
    "        # In case word doesn't exist in vocab return (0, \"كلمةمجهولة\")\n",
    "        words_list = [\"كلمةمجهولة\"] + list(words)\n",
    "        id_to_word = defaultdict(lambda: \"كلمةمجهولة\", {idx: value for idx, value in enumerate(words_list)})\n",
    "        word_to_id = defaultdict(lambda: 0, {value: idx for idx, value in enumerate(words_list)})\n",
    "        return words, id_to_word, word_to_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_trigrams) if self.is_train else len(self.test_trigrams)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trigrams = self.train_trigrams if self.is_train else self.test_trigrams\n",
    "        trigram = [ self.word_to_id[word] for word in trigrams[idx]]\n",
    "        return tuple(trigram)\n",
    "    \n",
    "    def get_word_from_id(self, idx):\n",
    "        return self.id_to_word[idx]\n",
    "    \n",
    "    def get_word_id(self, word):\n",
    "        return self.word_to_id[word]\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id of unkown word: 0\n",
      "ًWord of unkown id: كلمةمجهولة\n",
      "Vocab size: 184196\n",
      "Item 0 in dataset: [ 31643 - 2141] -> 89859\n",
      "Item 0 in words: [إيفو - أندريتش] -> من\n",
      "Item 1 in dataset: [ 89859 - 31643] -> 40309\n",
      "Item 1 in words: [أندريتش - من] -> الروائيين\n",
      "Item 2 in dataset: [ 40309 - 89859] -> 158358\n",
      "Item 2 in words: [من - الروائيين] -> القلائل\n"
     ]
    }
   ],
   "source": [
    "# Test dataset\n",
    "\n",
    "train_dataset = MyDataset(\"./dataset/alaraby1k.json\", is_train= True)\n",
    "test_dataset = MyDataset(\"./dataset/alaraby1k.json\", is_train= False)\n",
    "\n",
    "\n",
    "# Note that we defined an \"unkown word\" in the vocab dictionary so that the embedding\n",
    "# doesn't break when it encounter an unkown word. Now unkown words are mapped to id=0\n",
    "# and the value of id=0 is 'كلمةمجهولة\"'\n",
    "print(f\"Id of unkown word: {train_dataset.get_word_id('كلمة_مجهولة')}\")\n",
    "print(f\"ًWord of unkown id: {train_dataset.get_word_from_id(-1)}\")\n",
    "print(f\"Vocab size: {train_dataset.get_vocab_size()}\")\n",
    "\n",
    "item = 0\n",
    "x1, x2, y = train_dataset.__getitem__(item)\n",
    "print(f\"Item {item} in dataset: [ {x2} - {x1}] -> {y}\")\n",
    "print(f\"Item {item} in words: [{train_dataset.get_word_from_id(x1)} - {train_dataset.get_word_from_id(x2)}] -> {train_dataset.get_word_from_id(y)}\")\n",
    "\n",
    "item = 1\n",
    "x1, x2, y = train_dataset.__getitem__(item)\n",
    "print(f\"Item {item} in dataset: [ {x2} - {x1}] -> {y}\")\n",
    "print(f\"Item {item} in words: [{train_dataset.get_word_from_id(x1)} - {train_dataset.get_word_from_id(x2)}] -> {train_dataset.get_word_from_id(y)}\")\n",
    "\n",
    "item = 2\n",
    "x1, x2, y = train_dataset.__getitem__(item)\n",
    "print(f\"Item {item} in dataset: [ {x2} - {x1}] -> {y}\")\n",
    "print(f\"Item {item} in words: [{train_dataset.get_word_from_id(x1)} - {train_dataset.get_word_from_id(x2)}] -> {train_dataset.get_word_from_id(y)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Neural Network\n",
    "\n",
    "We will be using `nn.embedding` layer from pytroch. Internally the layer is a weight matrix of\\\n",
    "the following size `(embedding size, vocab size)`. The `embedding size` it the size of vecotr\\\n",
    "we want to represent the words with it is arbitrary chosen and optimized using hyperparameter\\\n",
    "optimization. The `vocab size` is the size of the vocab or the number of words we have.\\\n",
    "The matrix is updated and optimized using back propagation.\n",
    "\n",
    "Our vocab size is 197285 we wil use an embedding of vocab size 200,000. And we will\\\n",
    "choose the embedding size to be `512`. Which means each word will be represented using\\\n",
    "an a vector of size `512`. Given the complex nature of languages in general \\\n",
    "an given the hard nature of the given task, one should use a large embedding size\\\n",
    "this gives the model more capacity to learn more and more about the semantics of the data.\n",
    "\n",
    "Finally, be informed that `nn.embedding` is pretty similar to `nn.linear` layer.\\\n",
    "With a main difference that embedding takes words indices in the vocabulary\\\n",
    "while the linear layer takes one-hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class WordPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(WordPredictor, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim*2, vocab_size)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        embedded1 = self.embedding(x1)\n",
    "        embedded2 = self.embedding(x2)\n",
    "        concatenated = torch.cat((embedded1, embedded2), dim=1)\n",
    "        output = self.linear(concatenated)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Embedding\n",
    "\n",
    "In this code chunk we simply train the network given a number of hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" ) if torch.cuda.is_available() else torch.device(\"cpu\" )\n",
    "\n",
    "batch_size= 1500\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "vocab_size = 200_000\n",
    "embbeding_dim = 512\n",
    "model = WordPredictor(vocab_size, embbeding_dim)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "chkpnt_path = f\"word_predictor_{vocab_size}_{embbeding_dim}.chk\"\n",
    "if os.path.exists(chkpnt_path):\n",
    "  model.load_state_dict(torch.load(chkpnt_path))\n",
    "\n",
    "\n",
    "progress_path = \"/word_predictor_progress.json\"\n",
    "if os.path.exists(progress_path):\n",
    "  progress = json.load(open(progress_path, \"r\"))\n",
    "else:\n",
    "  progress = {\"chkpnt\" : 0, \"progress\" : []}\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in (progress[\"chkpnt\"], epochs):\n",
    "  for i, batch in enumerate(train_dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    x1, x2, target = batch\n",
    "    x1 = x1.to(device)\n",
    "    x2 = x2.to(device)\n",
    "    target = target.to(device)\n",
    "    output = model(x1, x2)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if(i % 10==0):\n",
    "      print(f\"Epoch {epoch } Batch {i}, Loss: {loss.item()}\")\n",
    "  progress[\"progress\"].append({\"epoch\": epoch, \"loss\": loss.item()})\n",
    "  progress[\"chkpnt\"] = epoch   \n",
    "  json.dump(progress, open(progress_path, \"w\"))\n",
    "  torch.save(model.state_dict(), chkpnt_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

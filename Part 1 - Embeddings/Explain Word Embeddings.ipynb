{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors of \"Attention is All You Need\" stated in the beginning of the paper: \\\n",
    "Similarly to other sequence transduction models, we use learned embeddings to\\\n",
    "convert the input tokens and output tokens to vectors of dimension d_model.\\\n",
    "What are word embeddings? Word embeddings are a way to represent textual data\\\n",
    "in terms of condensed real-valued vectors. Embeddings in general is the process\\\n",
    "of creating an euclidean space that represents a group of data mathematically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why not use indexes\n",
    "Well if I have to represent a set of words with numbers why not simply\\\n",
    "use indexes like this.\n",
    "\n",
    "\n",
    "[\"Being\", \"Strong\", \"Is\", \"All\", \"What\", \"Matters\"] -> [1,2,3,4,5,6].\n",
    "\n",
    "\n",
    "Well, we will lose meaning like this. I want to deal with words like numbers \\\n",
    "but without losing meaning. I want when I do this ``` King - Man ``` to get \\\n",
    "```Queen```. Is this possible yes it is possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meaning as feature\n",
    "We can achive the afromentioned behaviour by mapping each word to a set of features.\\\n",
    "Consider the following table where we have a set of words and a set of features.\\\n",
    "In it we are representing each word with a set of \"meaning\" features.\n",
    "\n",
    "A \"granny\" is an old weak female.\n",
    "\n",
    "\n",
    "| Word   | Old | Male | Weak\n",
    "|--      |--   |--    | -- \n",
    "|Grandpa | 1   | 1    | 1\n",
    "|Granny  | 1   | 0    | 1\n",
    "|Man     | 0   | 1    | 0\n",
    "|Woman   | 0   | 0    | 0\n",
    "|Mother  | 0   | 0    | 0\n",
    "|Son     | 0   | 1    | 1\n",
    "\n",
    "\n",
    "Consider this:\n",
    "\n",
    "$$\n",
    "Grandpa - Man = \n",
    "\n",
    "\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    1 \\\\\n",
    "    1 \\\\\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "\\end{bmatrix}\n",
    "= Granny\n",
    "$$\n",
    "\n",
    "You see. What we are doing is subtracting the manliness features from \"grandpa\"\\\n",
    "which will lead to \"granny\". Now those are embeddings. The feature vectors. They represent words\\\n",
    "and the relation between them. The question is how to get those embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Embedding\n",
    "\n",
    "To train an embedding we train a neural network to solve a task that requires\\\n",
    "semantical understanding of words and it will eventually have a representation or\\\n",
    "an embedding that holds semantical meaning.\n",
    "\n",
    "For more clarity consider a dataset with the following entries:\n",
    "\n",
    "* sam, altman -> is\n",
    "* altman, is -> the\n",
    "* is, the -> CEO\n",
    "* the, CEO -> of\n",
    "* CEO, of -> the\n",
    "* of, the, -> most\n",
    "* ...\n",
    "\n",
    "It is composed of trigrams of a given text. If we trained a simple feedforward neural network\\\n",
    "to predict the next word from the previous two words. The weights of the external layer of the\\\n",
    "network will eventually be the transformation function that give us our embeddings.\\\n",
    "$embedding(x) = Wx$ where W is the neural network external weights.\n",
    "\n",
    "Why we get semantical representation. Consider this word ```The commander ordered```.\\\n",
    "For the network to predict \"ordered\" it should learn to correlate \"ordering\" with \"commander\".\\\n",
    "If we look closely to the embedding, we might also find that \"king\" is also correlated with ordering.\\\n",
    "Because they both the \"commander\" and the \"king\" occur in similar contexts.\\\n",
    "In the case both the embedding of \"king\" and \"commander\" will be somehow similar.\n",
    "\n",
    "In ```Generate Word Embeddings.ipynb``` notebook we will use pytorch\\\n",
    "to train a neural network on a similar task and see how things unfold."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

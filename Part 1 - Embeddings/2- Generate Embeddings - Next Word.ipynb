{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dislaimer\n",
    "This notebook requires knowledge in:\n",
    "* Python\n",
    "* Neural Networks\n",
    "* Pytorch Datasets and Modules\n",
    "* Machine Learning Process Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings Example\n",
    "The goal of this notebook is to have a hands-on experience of words embeddings.\\\n",
    "We will do the following:\n",
    "* Load a set of Arabic text as trigrams\n",
    "* Build a simple neural network\n",
    "* Train the network to **predict next word**\n",
    "* Use the network weights as embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Dataset\n",
    "\n",
    "To train an embedding, we scraped a famous Arabic magazine and collected 1000 articles. The articles are\\\n",
    "found in the `alaraby1k.json` file. The task we will use to train the embedding is to predict a third word\\\n",
    "given two previous words. The task is framed as a straightforward classification task. The model should\\\n",
    "predict the id of the third word. Assuming we have approx. 165,000 words. Then we have 165,000 classes in\\\n",
    "the outer layer. Each entry in the dataset contains the ```author``` name the ```issue``` number\\\n",
    "and the article ```text```.\n",
    "\n",
    "The below code chunk loads the text of all the articles and generate all possible trigrams.\\\n",
    "We encapsulate the dataset with a customer pytorch ```Dataset``` class.\n",
    "\n",
    "In every transformer training process, we need to have a vocab. The vocab is the\\\n",
    "unique words that are data is composed of.\n",
    "\n",
    "**During the dataset initialization, we find all unique words and store them in a set** and\\\n",
    "then define two dictionaries that map a given vocabulary to its index and the second\\\n",
    "to map a given index to its vocabulary. We need the dictionaries because we read words, but we feed\\\n",
    "the network indices. We also get numbers, i.e., indices, from the network output, but we need words.\\\n",
    "Thus, those dictionaries.\n",
    "\n",
    "The network will eventually be trained, and we will run some tests, and it will happen\\\n",
    "that a word in the test set is not even in the vocabulary of the model. We need to map\\\n",
    "this word to an index and a common word for an unknown word.\\\n",
    "**we added to the vocab the word `<UNKOWN>` of index zero to replace words not in the vocab.** Check `__compute_vocab__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, alaraby_filepath, is_train):\n",
    "        self.raw_data = [article[\"text\"] for article in json.load(open(alaraby_filepath, \"r\"))]\n",
    "        self.train_raw_data, self.test_raw_data = train_test_split(self.raw_data, test_size=0.1, random_state=42, shuffle=False)\n",
    "        self.train_trigrams =  self.__generate_trigrams__(self.train_raw_data)\n",
    "        self.test_trigrams =  self.__generate_trigrams__(self.test_raw_data)\n",
    "        self.vocab, self.id_to_word, self.word_to_id = self.__compute_vocab__(self.train_raw_data)\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __generate_trigrams__(self, texts):\n",
    "        trigrams = []\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            article_trigrams = [words[i:i+3] for i in range(len(words)-2)]\n",
    "            trigrams+= article_trigrams\n",
    "        return trigrams\n",
    "\n",
    "    def __compute_vocab__(self, texts):\n",
    "        words = {}\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                words[word]= None\n",
    "        words_list = [\"<UNKNOWN>\"] + list(words)\n",
    "        id_to_word = defaultdict(lambda: \"<UNKNOWN>\", {idx: value for idx, value in enumerate(words_list)})\n",
    "        word_to_id = defaultdict(lambda: 0, {value: idx for idx, value in enumerate(words_list)})\n",
    "        return words, id_to_word, word_to_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_trigrams) if self.is_train else len(self.test_trigrams)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trigrams = self.train_trigrams if self.is_train else self.test_trigrams\n",
    "        trigram = [ self.word_to_id[word] for word in trigrams[idx]]\n",
    "        return tuple(trigram)\n",
    "\n",
    "    def get_word_from_id(self, idx):\n",
    "        return self.id_to_word[idx]\n",
    "\n",
    "    def get_word_id(self, word):\n",
    "        return self.word_to_id[word]\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id of unkown word: 0\n",
      "ًWord of unkown id: <UNKNOWN>\n",
      "Vocab size: 58912\n",
      "Item 0 in dataset: [ 10840 - 4707] -> 53070\n",
      "Item 0 in words: [إيفو - أندريتش] -> من\n",
      "Item 1 in dataset: [ 53070 - 10840] -> 41406\n",
      "Item 1 in words: [أندريتش - من] -> ال+\n",
      "Item 2 in dataset: [ 41406 - 53070] -> 35274\n",
      "Item 2 in words: [من - ال+] -> روائي\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MyDataset(\"../Dataset/alaraby1k.json\", is_train= True)\n",
    "test_dataset = MyDataset(\"../Dataset/alaraby1k.json\", is_train= False)\n",
    "\n",
    "print(f\"Id of unkown word: {train_dataset.get_word_id('<UNKNOWN>')}\")\n",
    "print(f\"ًWord of unkown id: {train_dataset.get_word_from_id(-1)}\")\n",
    "print(f\"Vocab size: {train_dataset.get_vocab_size()}\")\n",
    "\n",
    "item = 0\n",
    "x1, x2, y = train_dataset.__getitem__(item)\n",
    "print(f\"Item {item} in dataset: [ {x2} - {x1}] -> {y}\")\n",
    "print(f\"Item {item} in words: [{train_dataset.get_word_from_id(x1)} - {train_dataset.get_word_from_id(x2)}] -> {train_dataset.get_word_from_id(y)}\")\n",
    "\n",
    "item = 1\n",
    "x1, x2, y = train_dataset.__getitem__(item)\n",
    "print(f\"Item {item} in dataset: [ {x2} - {x1}] -> {y}\")\n",
    "print(f\"Item {item} in words: [{train_dataset.get_word_from_id(x1)} - {train_dataset.get_word_from_id(x2)}] -> {train_dataset.get_word_from_id(y)}\")\n",
    "\n",
    "item = 2\n",
    "x1, x2, y = train_dataset.__getitem__(item)\n",
    "print(f\"Item {item} in dataset: [ {x2} - {x1}] -> {y}\")\n",
    "print(f\"Item {item} in words: [{train_dataset.get_word_from_id(x1)} - {train_dataset.get_word_from_id(x2)}] -> {train_dataset.get_word_from_id(y)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Neural Network\n",
    "\n",
    "**The network task is framed as a straightforward classification task.** The model should predict\\\n",
    "the id of the third word. Assuming we have approx. 60,000 words. Then we have 60,000 classes\\\n",
    "in the outer layer.\n",
    "\n",
    "We will be using `nn.embedding` layer from pytroch. Internally the layer is a weight matrix of\\\n",
    "the following size `(embedding size, vocab size)`. The `embedding size` it the size of vecotr\\\n",
    "we want to represent the words with it is arbitrary chosen and optimized using hyperparameter\\\n",
    "optimization. The `vocab size` is the size of the vocab or the number of words we have.\\\n",
    "The matrix is updated and optimized using back propagation.\n",
    "\n",
    "Our vocab size is 58912 we will use an embedding of vocab size 60,000. And we will\\\n",
    "choose the embedding size to be `1024`. Which means each word will be represented using\\\n",
    "an a vector of size `1024`. Given the complex nature of languages in general \\\n",
    "an given the hard nature of the given task, one should use a large embedding size\\\n",
    "this gives the model more capacity to learn more and more about the semantics of the data.\n",
    "\n",
    "Finally, be informed that `nn.embedding` is pretty similar to `nn.linear` layer.\\\n",
    "With a main difference that embedding takes words indices in the vocabulary\\\n",
    "while the linear layer takes one-hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NextWordPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(NextWordPredictor, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim*2, vocab_size)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        embedded1 = self.embedding(x1)\n",
    "        embedded2 = self.embedding(x2)\n",
    "        concatenated = torch.cat((embedded1, embedded2), dim=1)\n",
    "        output = self.linear(concatenated)\n",
    "        return output\n",
    "\n",
    "    def embed(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "    def word(self, x):\n",
    "      distance = torch.norm(self.embedding.weight.data - x, dim=1)\n",
    "      nearest = torch.argmin(distance)\n",
    "      return nearest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Embedding\n",
    "\n",
    "In this code chunk we simply train the network given a number of hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" ) if torch.cuda.is_available() else torch.device(\"cpu\" )\n",
    "root= \"./\"\n",
    "\n",
    "batch_size= 1500\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "vocab_size = 60_000\n",
    "embbeding_dim = 128\n",
    "model = NextWordPredictor(vocab_size, embbeding_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "chkpnt_name = f\"next_word_predictor_{vocab_size}_{embbeding_dim}.chk\"\n",
    "chkpnt_path = os.path.join(root, chkpnt_name)\n",
    "if os.path.exists(chkpnt_path):\n",
    "  model.load_state_dict(torch.load(chkpnt_path))\n",
    "model.to(device)\n",
    "\n",
    "progress_name = \"next_word_predictor_progress.json\"\n",
    "progress_path = os.path.join(root, progress_name)\n",
    "if os.path.exists(progress_path):\n",
    "  progress = json.load(open(progress_path, \"r\"))\n",
    "else:\n",
    "  progress = {\"chkpnt\" : -1, \"progress\" : []}\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(progress[\"chkpnt\"]+1, epochs):\n",
    "  for i, batch in enumerate(train_dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    x1, x2, target = batch\n",
    "    x1 = x1.to(device)\n",
    "    x2 = x2.to(device)\n",
    "    target = target.to(device)\n",
    "    output = model(x1, x2)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if(i % 10==0):\n",
    "      print(f\"Epoch {epoch } Batch {i}, Loss: {loss.item()}\")\n",
    "  progress[\"progress\"].append({\"epoch\": epoch, \"loss\": loss.item()})\n",
    "  progress[\"chkpnt\"] = epoch\n",
    "  json.dump(progress, open(progress_path, \"w\"))\n",
    "  chkpnt_name = f\"next_word_predictor_{vocab_size}_{embbeding_dim}.chk\"\n",
    "  torch.save(model.state_dict(), os.path.join(root, chkpnt_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpnt = \"next_word_predictor_60000_128.chk\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" ) if torch.cuda.is_available() else torch.device(\"cpu\" )\n",
    "\n",
    "x1 = torch.as_tensor([ train_dataset.__getitem__(i)[0] for i in range(1500)]).to(device)\n",
    "x2 = torch.as_tensor([ train_dataset.__getitem__(i)[1] for i in range(1500)]).to(device)\n",
    "target = torch.as_tensor([ train_dataset.__getitem__(i)[2] for i in range(1500)]).to(device)\n",
    "\n",
    "inference = NextWordPredictor(60_000, 128).to(device)\n",
    "inference.load_state_dict(torch.load(chkpnt, map_location=device))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "output = inference(x1, x2)\n",
    "loss = criterion(output, target)\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "chkpnt = \"next_word_predictor_60000_128.chk\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" ) if torch.cuda.is_available() else torch.device(\"cpu\" )\n",
    "\n",
    "x1 = torch.as_tensor(train_dataset.word_to_id[\"جبر\"]).view(1).to(device)\n",
    "x2 = torch.as_tensor(train_dataset.word_to_id[\"لها\"]).view(1).to(device)\n",
    "\n",
    "inference = NextWordPredictor(60_000, 128).to(device)\n",
    "inference.load_state_dict(torch.load(chkpnt, map_location=device))\n",
    "\n",
    "logits = inference(x1, x2).detach().cpu().numpy()[0]\n",
    "word_ids = logits.argsort()[::-1][:3]\n",
    "\n",
    "print(f\"Word 1: {train_dataset.id_to_word[word_ids[0]]}\")\n",
    "print(f\"Word 2: {train_dataset.id_to_word[word_ids[1]]}\")\n",
    "print(f\"Word 3: {train_dataset.id_to_word[word_ids[2]]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
